This doc is just for writing up summaries inline with the links:

- [Are People Polite to Computers? Responses to Computer-Based Interviewing Systems](http://dx.doi.org/10.1111/j.1559-1816.1999.tb00142.x)

- Prairie Rose: [How people treat computers, television, and new media like real people and places](http://www.humanityonline.com/docs/the\%20media\%20equation.pdf)
  - Technology needs to be designed in a way that does not violate social conventions.  Doing so makes media appear technologically deficient and viewed poorly even if the user is unaware of why.  Design guidelines should include Grice’s Maxims: quality, quantity, relevance, and clarity.  Additionally, the input and output modalities should match.  For example, if typing input, the best user experience is text output rather than a text-to-speech audio response.

- Adam: [Cross-cultural studies of the computers are social actors paradigm: The case of reciprocity](http://web.ics.purdue.edu/~duffy/IE486_Spr07/ComputersAsSocialActor.pdf)

  - The motivation of this study is to investigate an important implicit bias of the existing Computers as Social Actors (CASA) studies: all previous research was done in the United States with U.S. participants, but the discoveries were generalized to all cultures. It is possible that at least some discovered behavior is dependent on culture. This paper details a comparative study between U.S and Japanese participants to evaluate if the social rules derived from each culture affects participants responses to computers. The researchers investigate the concept of *reciprocity*, defined as "the rule that people should help those who help them". Importantly, the social norms of reciprocity are viewed differently in American and Japanese culture. American participants exhibited behavior consisent with an individualist culture, while the Japanese participants were consisten with collectivist cultural behaviors. From this, the study's results show that computers are not just social actors, but *culturally-embedded* social actors. This means that computers are treated using the social norms for treating people *within the culture*. 

- Prairie Rose [Race in the live and the virtual interview: Racial deference, social desirability, and activation effects in attitude surveys](http://www.jstor.org/stable/1519835?seq=1#page_scan_tab_contents)
  - Attitude surveys are often use to measure the political climate around race relations in the United States.  However, social politeness factors often cause biases that cannot be avoided.  People answer differently based on the race of the interviewer on controversial issues, but this effect is not present on questions about more traditional attitudes about race.  This effect gives researchers a “social microscope” that may be a better indicator of racial attitudes than the survey itself.  These effects are also present in a virtual interview where the interviewer is a computer with a personified image of either a Caucasian or African-American person. 

- Adam: [Sympathetic guitar: humans respond socially to interactive technology in an abstract, expressive context](http://dl.acm.org/citation.cfm?id=2030443)

  - The motivation of this study is to determine how the quality of interaction with a computer can alter the sociability and politeness of humans towards that computer. Most studies (Nass, etc) provide information focused tasks to participants which involve direct interaction with visual media. New forms of interaction have been introduced by video games, creative authoring tools, and digital music instruments which are indirect, remote, or expressive in nature. The study investigates if these interaction types elicit similar social attributions, by using a digital guitar with responsive and laggy interaction. It exists as a theoretical replication of the original Nass study, performing the studying in an audio modality (instead of visual). It also suggests that any type of interactivity of a system will lead to some level of pre-conscious social behavior during use. 

- Adam: [Can a retail web site be social?](http://journals.ama.org/doi/abs/10.1509/jmkg.71.3.143)

  - Our understanding of computer systems has evolved as time has progressed. Many people do not view what is seen on a computer screen as "the computer" anymore, but instead as a program or service provided remotely (another computer elsewhere). One such service is a retail website. This paper studies the use of lifelike characters, called avatars, in online retail websites. The motivation of the study is to investigate how the social cues created by the existence of avatars influence consumer behavior by imbueing the website with a social quality. The study empirically shows that customer-employee interactions found in real world stores can be induced by social cues provided by website avatars. 

- Adam: [Effects of mental model quality on collaborative system performance](http://pro.sagepub.com/content/51/22/1506.short)

  - In order to create better software, it is important to study the attitudes of software developers and how they view their work environments. Given software developers’ advanced understanding of how computers work, one would expect the attribution of social qualities to computers to be minimal or not present if the quality of an individual's mental model of a computer system modifies the sociability of a computer system. This master's thesis studies the affect of mental model quality on maintaining teammate bonds with automated computer systems. It finds that human/computer team performance is higher when human users have accurate mental models. It suggests (but doesn't study or prove) that this increased success rate, improves the human's trust and confidence in the computer system. 

- Shaown: [Machines and Mindlessness: Social Responses to Computers](http://dx.doi.org/10.1111/0022-4537.00153)
  
  __Summary__: Social stereotypes extending to responses to and interaction with computers:
      1. _Gender_: Evaluation from male-voice is more appreciated and considered more friendly than a female-voice evaluator, despite their contents being identical. Praisal from the male-voice computer was more compelling than the female-voiced one. Female computer considered more informative about love and relationships compared to male-voiced tutor which was considered more informative about computers.
      2. _Ethnicity_: Participants in the same ethnicity of that of the computer emulated perceived the computer agent to be more attractive, trustworthy, persuasive, and intelligent - making decision more similar to them, compared to different ethnicity agent participant condition.
      3. _Group loyalty_: People cooperate and conform to the computer’s suggestion, assess the computer as more friendly and more intelligent, perceives the computer as being similar to themselves, compared to participants in the non-team condition despite completely understanding that the computer is not able to return the participant’s loyalty.
      4. _Deeply ingrained habits and behaviors (overlearning)_: politeness, reciprocity, reciprocal self-disclosure are also mindlessly extended towards computers.
    
    __Linkage to our paper__:
    There have been studies that focus on the human application of social norms and expectations to computers in a seamless manner. Nass and Moon performed a series of experiments that pointed out that social stereotypes attributed to gender, ethnicity, and group loyalty, along with deeply ingrained social habits and behaviors like, reciprocity and reciprocal self-disclosure, are extended to computers when interacting with them.

- Matt Neal: [How low can you go? Ostracism by a computer is sufficient to lower self-reported levels of belonging, control, self-esteem, and meaningful existence](http://www.sciencedirect.com/science/article/pii/S0022103103001823)
  - This was a paper that cited Nass.  It basically tried to show how strong ostracism is as a psychological mechanism.  They used a web based ball throwing game where there were three participants.  One would be a human participant, and the other two would be either human or computer agents.  The study used a good amount of trickery where in all treatments the participants played against computer agents, the only difference was whether the computer agents would involve the human participant in the game or not.  If the participant was supposed to be playing with two other users they would call other experimenters and pretend that they were setting up another user in another location.  The result was that users feelings of ostracism when the computer didn't throw the ball were equivalent to the feelings they had when other "human" agents did the same thing.  I'm not totally sure this one is useful either, but this is what I would likely say about it:
  - "In Zadrow's paper on ostracism with human and computer agents it was shown that in fact humans can have strong and interesting social reactions to the behavior of computer agents.  In their study they showed that in a relatively pointless game humans can feel ostracised by a computer agent that simply does not throw them the ball in a computer game to the same level that they feel ostracised by human agents.  The one limiting factor that is that, as Zadrow mentions, ostracism is a deeply-felt social mechanism for humans influenced by evolutionary factors in being that it may be considered to be more likely that humans would react to ostracism even with computers."

- Shaown: [Establishing and Maintaining Long-term Human-computer Relationships](http://doi.acm.org/10.1145/1067860.1067867)

  __Summary__: Created relational computer agent for long term relation creation and maintenance with their users. Participants who interacted on a daily basis with the relational agent respected, trusted, and liked the relational agent more compared to a non-relational task based agent.
  
  __Linkage to our paper__: With the prevalence of computers throughout society, researchers have also been exploring the factors of creating and maintaining long-term relations between computer and their users. Bickmore and Picard, though their study, found out that human users of a relational computer agent did not only respect, trust, and like the agent, but also wanted to continue working with the agent even after the end of the study. In this work, we investigate whether such long term relationship between a human user and her computer has any significant effect on the social norm of politeness witnessed by Nass et al in their original study.


- Matt Neal: [Responses to robot social roles and social role framing](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5928687)

  - Another paper which has nass included in the authorship.  The paper is about robots helping people in dangerous circumstances.  The main suggested situation is something like a mine collapse.  I'm not totally sure that this is applicable to our paper other than in the following way:
  - "Work in this area has continued and has moved away from a direct discussion of whether people personify computers or not into the area of the social aspect of robots.  One of Nass's later papers (cite) evaluates whether the social role of a particular robot has an effect on how well it can perform its job.  The job for this case was search and rescue robots to deal with injured people in circumstances where human's could not reach them. They considered robots that were nothing more than relays of information from a human controller, those that took on a role of being a sort of pseudo-AI that behaved as if it was a real social actor, and a hybrid.  Nass' early work on the social constructs around human's perceptions of computers provided the foundation for this work, in that although robots can appear in a more humanoid form they are still at their core computers and establishing the plausibility of users looking at machines as social createures was a necessary percursor to this later work"

- Matt Neal: [Media Equation Revisited: Do Users Show Polite Reactions towards an Embodied Agent?](http://dx.doi.org/10.1007/978-3-642-04380-2_19)

  - This paper is essentially a theoretical replication of the initial Nass study.  They made a few changes:
      -  There isn't a tutor, there is this AI called Max, he's poorly made CGI guy who they showed on a screen at life size.  Max speaks directly to the participants via speakers and they speak directly to him.  
      -  Apparently they either lacked the time or inclination to put together any natural language processing so they used what they called a "Wizard of Oz" approach, where they had some hidden guy inputting the participants responses into a machine.
      -  They wanted to see if people thought of the programmer of the ai when being polite.
      -  They had three conditions for conducting the survey at the end:  
         - Max asks the questions and the user responds via speech
         - The participant answers a pen and paper questionaire in front of max who is doing some idle behavior.
         - The participant answers a pen and paper questionaire outside of the room.
    -  The paper has a ton of references to other papers that are in this area, some of which could be really useful.  In particular Nass later conducted a study that tried to determine whether participants were thinking about programmers when being polite to the machine.
    -  My blurb is below:
        - "Hoffmann's paper (cite) makes a clear attempt at a theoretical replication without ever really stating that as its aim.  They used an AI agent that was displayed at human size and spoke to participants and asked questions.  They had a very similar structure as far as evaluating the participant's politeness to the machine:  The user would either evaluate the agent by answering its questions directly, would answer a questionaire on pen and paper before the agent, or would answer a questionaire in a room separately from the agent.  Hoffmann additionally looked at a factor that we considered when beginning our work:  Do participants consider the programmer of the "AI" when evaluating the "AI".  Hoffman found that in fact users do think about that factor, and it makes a difference in terms of their responses in that they behave more politely towards the agent at least with respect to considering its competence.  Those results were in conflict with results provided by a paper that Nass' group wrote, that found that users do not consider the programmer when evaluating the agent. (There's really not much to read on this other supposed paper it's referenced in the paper [here](http://onlinelibrary.wiley.com.prox.lib.ncsu.edu/doi/10.1111/0022-4537.00153/abstract) by nass as well as: "Are Programmers Psychologically Relevant to Human-Computer Interaction? Paper presented at the annual meeting of the International Communication Association, San Francisco, CA (1994)"  Seems fair to assume that it was never published.)"

- Prairie Rose: [On the difficulty of replicating human subjects studies in software engineering](http://dx.doi.org/10.1145/1368088.1368115)
  - Exact replications of human experiments are incredibly difficult to perform, because a lot of information is tacit and will not be found in written materials.  Contact with the original authors is almost always necessary.  Differences will inevitably occur, and should be accounted for individually so that results can be directly compared with the original experiment.  Replications should be done to strengthen the original findings or to find biases or errors in the original experiment that can be improved upon. 

- Prairie Rose [Are computers scapegoats? Attributions of responsibility in human computer interaction](http://www.sciencedirect.com/science/article/pii/S1071581998901999)

  - Because Human-Computer interaction has two sides, responsibility can be placed on either or both sides of the interactions.  Researchers created an experiment in which they analyzed participants as either dominant or submissive personalities, and then they completed a desert survival scenario.  The language in the scenario was manipulated to use dominant or submissive language.  When the outcome of the scenario was negative, participants who used a computer with a similar "personality" were less likely to blame the computer and more likely to blame themselves.  When outcome was positive, participants working with a similar "personality" computer were more likely to credit the computer rather than themselves.  In addition, when users were given more control, useres were more likely to attribute the outcome to themselves.  
